{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagiri262/DataMining-2025Fall-MUST/blob/main/Assignment2_2250016066.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWZABc8DM3sk"
      },
      "source": [
        "# Assignment 2:\n",
        "# Mining Frequent Patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ux5MzJKbr8t"
      },
      "source": [
        "In this assignment, we will using Apriori algorithm to mine the frequent patterns (sort terms, just set k = 3, 4, 5) from a social network dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KccoTuxW5FR"
      },
      "source": [
        "##Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_nDDkVgW8OZ"
      },
      "source": [
        "At first, download the dataset from http://snap.stanford.edu/data/gemsec_facebook_dataset.tar.gz and read the dataset description. Then upload and connect it to your VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjsWCUcMYC26",
        "outputId": "c1416571-a712-4447-c6c2-0a1b79c59a1c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COo-EbynXfca"
      },
      "source": [
        "In this pratice, we just use the first datafile (artist_edges.csv).\n",
        "\n",
        "Now load the data to a variable in python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdlZb2PeL2Jd"
      },
      "source": [
        "labdir = \"/content/sample_data/facebook_clean_data/\"\n",
        "import csv\n",
        "\n",
        "with open(labdir+\"tvshow_edges.csv\") as csvfile:\n",
        "    facebook_clean_data = list(csv.reader(csvfile))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_baMTKbndFJ2"
      },
      "source": [
        "##Apriori Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGVpq7mtdYDn"
      },
      "source": [
        "Apriori is an algorithm for frequent item set mining and association rule learning over relational databases.\n",
        "\n",
        "Before to perform Apriori, we need to define a function 'data_processing' for data preprocessing firstly.\n",
        "\n",
        "'data_set' is a list of transactions. Each transaction contains several items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIj8abpjff30"
      },
      "source": [
        "def processing_data():\n",
        "  dic = {}\n",
        "  for i in facebook_clean_data[1:]:\n",
        "    if not i[0] in dic:\n",
        "      dic[i[0]] = [i[1]]\n",
        "    else:\n",
        "      dic[i[0]].append(i[1])\n",
        "  data_set = list(dic.values())\n",
        "  return data_set\n",
        "data_set = processing_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSzV4BXdgK-n"
      },
      "source": [
        "Now define a function 'create_C1' ot create frequent candidate 1-itemset C1 by scaning data set.\n",
        "\n",
        "C1: A set which contains all frequent candidate 1-itemsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjaS_-4_gUA-"
      },
      "source": [
        "def create_C1(data_set):\n",
        "    C1 = set()\n",
        "    for t in data_set:\n",
        "        for item in t:\n",
        "            item_set = frozenset([item])\n",
        "            C1.add(item_set)\n",
        "    return C1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGlLBry6hZpP"
      },
      "source": [
        "Judge whether a frequent candidate k-itemset satisfy Apriori property.\n",
        "\n",
        "Ck_item: A frequent candidate k-itemset in Ck which contains all frequent candidate k-itemsets.\n",
        "\n",
        "Lksub1: A set which contains all frequent candidate (k-1)-itemsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS7Sd5kmhrIW"
      },
      "source": [
        "def is_apriori(Ck_item, Lksub1):\n",
        "    for item in Ck_item:\n",
        "        sub_Ck = Ck_item - frozenset([item])\n",
        "        if sub_Ck not in Lksub1:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbTWdnowh9O_"
      },
      "source": [
        "Create Ck, a set which contains all all frequent candidate k-itemsets\n",
        "    by Lk-1's own connection operation.\n",
        "\n",
        "k: the item number of a frequent itemset.\n",
        "\n",
        "Ck: A set which contains all all frequent candidate k-itemsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnDgTfe7iBd3"
      },
      "source": [
        "def create_Ck(Lksub1, k):\n",
        "# ToDo: Write you code here.\n",
        "    Ck = set()\n",
        "    list_Lksub1 = list(Lksub1)\n",
        "    len_Lksub1 = len(list_Lksub1)\n",
        "    for i in range(len_Lksub1):\n",
        "        for j in range(i+1, len_Lksub1):\n",
        "            L1 = list(list_Lksub1[i])[:k-2]\n",
        "            L2 = list(list_Lksub1[j])[:k-2]\n",
        "            L1.sort()\n",
        "            L2.sort()\n",
        "\n",
        "            # 仅当 (k-2) 个项相同时才进行连接\n",
        "            if L1 == L2:\n",
        "                # 连接操作：L1和L2的并集\n",
        "                Ck_item = list_Lksub1[i] | list_Lksub1[j]\n",
        "                # 剪枝\n",
        "                if is_apriori(Ck_item, Lksub1):\n",
        "                    Ck.add(Ck_item)\n",
        "    return Ck"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gb68FEHnxCW"
      },
      "source": [
        "Generate Lk by executing a delete policy from Ck.\n",
        "\n",
        "min_support: The minimum support.\n",
        "\n",
        "support_data: A dictionary. The key is frequent itemset and the value is support.\n",
        "\n",
        "Lk: A set which contains all all frequent k-itemsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUhsI7BjiEyg"
      },
      "source": [
        "def generate_Lk_by_Ck(data_set, Ck, min_support, support_data):\n",
        "# ToDo: Write you code here.\n",
        "    item_count = {}\n",
        "    Lk = set()\n",
        "    num_transactions = len(data_set)\n",
        "\n",
        "    # 遍历数据集计算 Ck 中每个候选项集的支持度计数\n",
        "    for t in data_set:\n",
        "        transcation = frozenset(t)\n",
        "        for item in Ck:\n",
        "            if item.issubset(transcation):\n",
        "                if item not in item_count:\n",
        "                    item_count[item] = item_count.get(item, 0) + 1\n",
        "\n",
        "    # 遍历候选项集，保留支持度大于等于 min_support 的项集，生成 Lk\n",
        "    for item, count in item_count.items():\n",
        "        support = count / num_transactions\n",
        "        if support >= min_support:\n",
        "            Lk.add(item)\n",
        "            support_data[item] = support\n",
        "\n",
        "    return Lk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n1SAMElojLn"
      },
      "source": [
        "Generate all frequent itemsets.\n",
        "\n",
        "k: Maximum number of items for all frequent itemsets.\n",
        "\n",
        "L: The list of Lk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN4l6hJ9iFOI"
      },
      "source": [
        "def generate_L(data_set, k, min_support):\n",
        "# ToDo: Write you code here.\n",
        "    support_data = {}\n",
        "    C1 = create_C1(data_set)\n",
        "    L1 = generate_Lk_by_Ck(data_set, C1, min_support, support_data)\n",
        "    L = [L1]\n",
        "\n",
        "    # 从 k = 2 开始循环，直到 Lk 为空或达到最大项数 k\n",
        "    for i in range(2, k+2):\n",
        "        Lksub1 = L[i-2]\n",
        "        Ck = create_Ck(Lksub1, i)\n",
        "\n",
        "        Lk = generate_Lk_by_Ck(data_set, Ck, min_support, support_data)\n",
        "\n",
        "        # 如果 Lk 为空，则停止循环\n",
        "        if not Lk:\n",
        "            break\n",
        "\n",
        "        L.append(Lk)\n",
        "    return L, support_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAKWDJKuowQN"
      },
      "source": [
        "Generate big rules from frequent itemsets.\n",
        "\n",
        "min_conf: Minimal confidence.\n",
        "\n",
        "big_rule_list: A list which contains all big rules. Each big rule is represented as a 3-tuple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpkKAi8KiGw3"
      },
      "source": [
        "def generate_big_rules(L, support_data, min_conf):\n",
        "# ToDo: Write you code here.\n",
        "    big_rule_list = []\n",
        "\n",
        "    for i in range(1, len(L)):\n",
        "        for freq_set in L[i]:\n",
        "            H1 = [frozenset([item]) for item in freq_set]\n",
        "            if i > 1:\n",
        "                rules_from_conseq(freq_set, H1, support_data, big_rule_list, min_conf)\n",
        "            else:\n",
        "                calc_conf(freq_set, H1, support_data, big_rule_list, min_conf)\n",
        "    return big_rule_list\n",
        "\n",
        "\n",
        "def calc_conf(freq_set, H, support_data, big_rule_list, min_conf):\n",
        "    pruned_H = []\n",
        "    for conseq in H:\n",
        "        # 计算置信度\n",
        "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
        "        if conf >= min_conf:\n",
        "            # 添加规则\n",
        "            big_rule_list.append((freq_set - conseq, conseq, conf))\n",
        "            pruned_H.append(conseq)\n",
        "    return pruned_H\n",
        "\n",
        "\n",
        "# 辅助函数：从 k-项集生成规则的后件\n",
        "def rules_from_conseq(freq_set, H, support_data, big_rule_list, min_conf):\n",
        "    m = len(H[0])\n",
        "    if len(freq_set) > (m + 1):\n",
        "        # H 是 m 项集后件的列表\n",
        "        # 从 m-项集后件生成 (m+1)-项集后件\n",
        "        Hmp1 = create_Ck(H, m + 1)\n",
        "        Hmp1 = calc_conf(freq_set, Hmp1, support_data, big_rule_list, min_conf)\n",
        "\n",
        "        # 如果还有可以进一步生成的后件，则递归调用\n",
        "        if len(Hmp1) > 1:\n",
        "            rules_from_conseq(freq_set, Hmp1, support_data, big_rule_list, min_conf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YtoZRBzMUYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb24165f-a46e-4c0a-e04f-f03ef1ae03de"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # 调用 processing_data 获取数据集\n",
        "    # 注意：这里使用了一个示例数据集，如果运行您的实际数据，请确保 facebook_clean_data 变量已定义\n",
        "    data_set = processing_data()\n",
        "\n",
        "    # 示例数据\n",
        "    # data_set = [['l1', 'l2', 'l5'], ['l2', 'l4'], ['l2', 'l3'], ['l1', 'l2', 'l4'], ['l1', 'l3'], ['l2', 'l3'],\n",
        "    #            ['l1', 'l3'], ['l1', 'l2', 'l3', 'l5'], ['l1', 'l2', 'l3']]\n",
        "\n",
        "    # 生成所有频繁项集\n",
        "    # k=3: 期望的最大频繁项集项数\n",
        "    # min_support=0.2: 最小支持度阈值\n",
        "    L, support_data = generate_L(data_set, k=3, min_support=0.2)\n",
        "\n",
        "    # 生成关联规则\n",
        "    # min_conf=0.7: 最小置信度阈值\n",
        "    big_rules_list = generate_big_rules(L, support_data, min_conf=0.7)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f\"生成的频繁项集列表 L 的长度: {len(L)}\")\n",
        "    print(f\"L[0] (L1) 中的项集数量: {len(L[0])}\")\n",
        "\n",
        "    for Lk in L:\n",
        "        print(\"=\"*50)\n",
        "        # 获取项集的大小\n",
        "        if Lk:\n",
        "            k_size = len(list(Lk)[0])\n",
        "            print(f\"频繁 {k_size}-项集\\t\\t支持度\")\n",
        "        else:\n",
        "            print(\"空集\")\n",
        "        print(\"=\"*50)\n",
        "        for freq_set in Lk:\n",
        "            # support_data[freq_set] 存储着支持度\n",
        "            print(freq_set, round(support_data[freq_set], 3))\n",
        "\n",
        "    print(\"\\n大规则 (关联规则)\")\n",
        "    for item in big_rules_list:\n",
        "        # item: (前件, 后件, 置信度)\n",
        "        print(f\"{set(item[0])} => {set(item[1])} \\t 置信度: {round(item[2], 3)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "生成的频繁项集列表 L 的长度: 1\n",
            "L[0] (L1) 中的项集数量: 0\n",
            "==================================================\n",
            "空集\n",
            "==================================================\n",
            "\n",
            "大规则 (关联规则)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vAxnJaOLAmx"
      },
      "source": [
        "data_set = processing_data()\n",
        "# data_set = [['l1', 'l2', 'l5'], ['l2', 'l4'], ['l2', 'l3'], ['l1', 'l2', 'l4'], ['l1', 'l3'], ['l2', 'l3'],\n",
        "#            ['l1', 'l3'], ['l1', 'l2', 'l3', 'l5'], ['l1', 'l2', 'l3']]\n",
        "L, support_data = generate_L(data_set, k=3, min_support=0.2)\n",
        "big_rules_list = generate_big_rules(L, support_data, min_conf=0.7)\n",
        "print(len(L))\n",
        "print(L[0])\n",
        "for Lk in L:\n",
        "  print(\"=\"*50)\n",
        "  print(\"frequent \" + str(len(list(Lk)[0])) + \"-itemsets\\t\\tsupport\")\n",
        "  print(\"=\"*50)\n",
        "  for freq_set in Lk:\n",
        "    print(freq_set, support_data[freq_set])\n",
        "print(\"Big Rules\")\n",
        "for item in big_rules_list:\n",
        "  print(item[0], \"=>\", item[1], \"conf: \", item[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can download data in this way:\n",
        "open the ternimal and enter \"wget http://snap.stanford.edu/data/gemsec_facebook_dataset.tar.gz\" download data to ./sample_data. Then use command \"tar -xvf gemsec_facebook_dataset.tar.gz\" unzip the data to folder."
      ],
      "metadata": {
        "id": "KSTHnYEOCWFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Apriori Alogrithm Codes\n",
        "\n",
        "import csv\n",
        "import itertools\n",
        "\n",
        "labdir = \"/content/sample_data/\"\n",
        "\n",
        "with open(labdir+\"tvshow_edges.csv\") as csvfile:\n",
        "    facebook_clean_data = list(csv.reader(csvfile))\n",
        "    # print(facebook_clean_data[:5])\n",
        "\n",
        "def processing_data():\n",
        "    dic = {}\n",
        "    for i in facebook_clean_data[1:]:\n",
        "        if not i[0] in dic:\n",
        "            dic[i[0]] = [i[1]]\n",
        "        else:\n",
        "            dic[i[0]].append(i[1])\n",
        "    data_set = list(dic.values())\n",
        "    return data_set\n",
        "data_set = processing_data()\n",
        "\n",
        "\n",
        "def create_C1(data_set):\n",
        "    C1 = set()\n",
        "    for t in data_set:\n",
        "        for item in t:\n",
        "            item_set = frozenset([item])\n",
        "            C1.add(item_set)\n",
        "    return C1\n",
        "\n",
        "\n",
        "def is_apriori(Ck_item, Lksub1):\n",
        "    for item in Ck_item:\n",
        "        sub_Ck = Ck_item - frozenset([item])\n",
        "        if sub_Ck not in Lksub1:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def create_Ck(Lksub1, k):\n",
        "# ToDo: Write you code here.\n",
        "    Ck = set()\n",
        "    list_Lksub1 = list(Lksub1)\n",
        "    len_Lksub1 = len(list_Lksub1)\n",
        "    for i in range(len_Lksub1):\n",
        "        for j in range(i+1, len_Lksub1):\n",
        "            L1 = list(list_Lksub1[i])[:k-2]\n",
        "            L2 = list(list_Lksub1[j])[:k-2]\n",
        "            L1.sort()\n",
        "            L2.sort()\n",
        "\n",
        "            # 仅当 (k-2) 个项相同时才进行连接\n",
        "            if L1 == L2:\n",
        "                # 连接操作：L1和L2的并集\n",
        "                Ck_item = list_Lksub1[i] | list_Lksub1[j]\n",
        "                # 剪枝\n",
        "                if is_apriori(Ck_item, Lksub1):\n",
        "                    Ck.add(Ck_item)\n",
        "    return Ck\n",
        "\n",
        "\n",
        "def generate_Lk_by_Ck(data_set, Ck, min_support, support_data):\n",
        "# ToDo: Write you code here.\n",
        "    item_count = {}\n",
        "    Lk = set()\n",
        "    num_transactions = len(data_set)\n",
        "\n",
        "    # 遍历数据集计算 Ck 中每个候选项集的支持度计数\n",
        "    for t in data_set:\n",
        "        transcation = frozenset(t)\n",
        "        for item in Ck:\n",
        "            if item.issubset(transcation):\n",
        "                if item not in item_count:\n",
        "                    item_count[item] = item_count.get(item, 0) + 1\n",
        "\n",
        "    # 遍历候选项集，保留支持度大于等于 min_support 的项集，生成 Lk\n",
        "    for item, count in item_count.items():\n",
        "        support = count / num_transactions\n",
        "        if support >= min_support:\n",
        "            Lk.add(item)\n",
        "            support_data[item] = support\n",
        "\n",
        "    return Lk\n",
        "\n",
        "\n",
        "def generate_L(data_set, k, min_support):\n",
        "# ToDo: Write you code here.\n",
        "    support_data = {}\n",
        "    C1 = create_C1(data_set)\n",
        "    L1 = generate_Lk_by_Ck(data_set, C1, min_support, support_data)\n",
        "    L = [L1]\n",
        "\n",
        "    # 从 k = 2 开始循环，直到 Lk 为空或达到最大项数 k\n",
        "    for i in range(2, k+2):\n",
        "        Lksub1 = L[i-2]\n",
        "        Ck = create_Ck(Lksub1, i)\n",
        "\n",
        "        Lk = generate_Lk_by_Ck(data_set, Ck, min_support, support_data)\n",
        "\n",
        "        # 如果 Lk 为空，则停止循环\n",
        "        if not Lk:\n",
        "            break\n",
        "\n",
        "        L.append(Lk)\n",
        "    return L, support_data\n",
        "\n",
        "\n",
        "def generate_big_rules(L, support_data, min_conf):\n",
        "# ToDo: Write you code here.\n",
        "    big_rule_list = []\n",
        "\n",
        "    for i in range(1, len(L)):\n",
        "        for freq_set in L[i]:\n",
        "            H1 = [frozenset([item]) for item in freq_set]\n",
        "            if i > 1:\n",
        "                rules_from_conseq(freq_set, H1, support_data, big_rule_list, min_conf)\n",
        "            else:\n",
        "                calc_conf(freq_set, H1, support_data, big_rule_list, min_conf)\n",
        "    return big_rule_list\n",
        "\n",
        "\n",
        "def calc_conf(freq_set, H, support_data, big_rule_list, min_conf):\n",
        "    pruned_H = []\n",
        "    for conseq in H:\n",
        "        # 计算置信度\n",
        "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
        "        if conf >= min_conf:\n",
        "            # 添加规则\n",
        "            big_rule_list.append((freq_set - conseq, conseq, conf))\n",
        "            pruned_H.append(conseq)\n",
        "    return pruned_H\n",
        "\n",
        "\n",
        "# 辅助函数：从 k-项集生成规则的后件\n",
        "def rules_from_conseq(freq_set, H, support_data, big_rule_list, min_conf):\n",
        "    m = len(H[0])\n",
        "    if len(freq_set) > (m + 1):\n",
        "        # H 是 m 项集后件的列表\n",
        "        # 从 m-项集后件生成 (m+1)-项集后件\n",
        "        Hmp1 = create_Ck(H, m + 1)\n",
        "        Hmp1 = calc_conf(freq_set, Hmp1, support_data, big_rule_list, min_conf)\n",
        "\n",
        "        # 如果还有可以进一步生成的后件，则递归调用\n",
        "        if len(Hmp1) > 1:\n",
        "            rules_from_conseq(freq_set, Hmp1, support_data, big_rule_list, min_conf)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 调用 processing_data 获取数据集\n",
        "    # 注意：这里使用了一个示例数据集，如果运行您的实际数据，请确保 facebook_clean_data 变量已定义\n",
        "    data_set = processing_data()\n",
        "\n",
        "    # 示例数据\n",
        "    # data_set = [['l1', 'l2', 'l5'], ['l2', 'l4'], ['l2', 'l3'], ['l1', 'l2', 'l4'], ['l1', 'l3'], ['l2', 'l3'],\n",
        "    #            ['l1', 'l3'], ['l1', 'l2', 'l3', 'l5'], ['l1', 'l2', 'l3']]\n",
        "\n",
        "    # 生成所有频繁项集\n",
        "    # k=3: 期望的最大频繁项集项数\n",
        "    # min_support=0.2: 最小支持度阈值\n",
        "    L, support_data = generate_L(data_set, k=3, min_support=0.2)\n",
        "\n",
        "    # 生成关联规则\n",
        "    # min_conf=0.7: 最小置信度阈值\n",
        "    big_rules_list = generate_big_rules(L, support_data, min_conf=0.7)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f\"生成的频繁项集列表 L 的长度: {len(L)}\")\n",
        "    print(f\"L[0] (L1) 中的项集数量: {len(L[0])}\")\n",
        "\n",
        "    for Lk in L:\n",
        "        print(\"=\"*50)\n",
        "        # 获取项集的大小\n",
        "        if Lk:\n",
        "            k_size = len(list(Lk)[0])\n",
        "            print(f\"频繁 {k_size}-项集\\t\\t支持度\")\n",
        "        else:\n",
        "            print(\"空集\")\n",
        "        print(\"=\"*50)\n",
        "        for freq_set in Lk:\n",
        "            # support_data[freq_set] 存储着支持度\n",
        "            print(freq_set, round(support_data[freq_set], 3))\n",
        "\n",
        "    print(\"\\n大规则 (关联规则)\")\n",
        "    for item in big_rules_list:\n",
        "        # item: (前件, 后件, 置信度)\n",
        "        print(f\"{set(item[0])} => {set(item[1])} \\t 置信度: {round(item[2], 3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "DLA1MUg0_90_",
        "outputId": "cc5582fb-7ee4-401e-d74e-53cc332d42e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/tvshow_edges.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3124593816.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlabdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/sample_data/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"tvshow_edges.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m    \u001b[0mfacebook_clean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0;31m# print(facebook_clean_data[:5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/tvshow_edges.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZegsx9ZCSWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}