{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagiri262/DataMining-2025Fall-MUST/blob/main/Assignment_3_2250016066.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ut5q2GiaeJ_"
      },
      "source": [
        "# Assignment 3: Similar document searching via MinHash and Locality Sensitive Hashing\n",
        "This Jupyter Notebook organizes the experiment task of \"Similar Document Retrieval based on MinHash and LSH\", presenting text descriptions (Text) and code frameworks (Code) separately without modifying the original content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACQJ2uTyaeKA"
      },
      "source": [
        "## 1. Experiment Overview (Text)\n",
        "In this project, we will implement the similar document retrieval system described in the lecture notes. This project is inspired by [http://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/](http://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/) (Note: You can refer to the code ideas on this website, but you need to implement it yourself)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3gs_ROcaeKA"
      },
      "source": [
        "### 1.1 Task Objective (Text)\n",
        "We will use documents from this repository: [http://www.inf.ed.ac.uk/teaching/courses/tts/assessed/assessment3.html](http://www.inf.ed.ac.uk/teaching/courses/tts/assessed/assessment3.html).\n",
        "This dataset contains 10,000 documents, and some of the document pairs are marked as plagiarism instances. The goal of this experiment is to verify whether the MinHash and LSH system can identify these plagiarism instances efficiently and accurately.\n",
        "\n",
        "Note: Smaller subsets of data suitable for testing can be obtained here: [https://github.com/chrisjmccormick/MinHash/tree/master/data](https://github.com/chrisjmccormick/MinHash/tree/master/data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZub76qsaeKB"
      },
      "source": [
        "## 2. Part I: Preliminaries\n",
        "**DUE**: Monday Sept. 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OibXhJvoaeKB"
      },
      "source": [
        "### 2.1 Part IA: Dataset parsing (Text)\n",
        "Write a function `parse_data` that takes the path to a filename, reads in the article data and returns an array of tuples. Requirements are as follows:\n",
        "- One tuple per article (there is one article per line).\n",
        "- Each tuple contains `(id, string)`, where `id` is the article id and `string` is the processed article text.\n",
        "- Article text processing rules:\n",
        "  1. Remove all punctuation.\n",
        "  2. Change all letters to lowercase.\n",
        "  3. Remove all whitespace so that all words are concatenated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "72wjrIcraeKB"
      },
      "outputs": [],
      "source": [
        "def parse_data(filename):\n",
        "  import string\n",
        "  from typing import List, Tuple\n",
        "  # read lines from filename\n",
        "  # construct tuple of id and text\n",
        "  # process string as described above\n",
        "  # return tuple with id and processed string\n",
        "  data_list: List[Tuple[str, str]] = []\n",
        "\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "  try:\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "          # 跳过空行\n",
        "          continue\n",
        "\n",
        "        parts = line.split(maxsplit=1)\n",
        "        if not parts:\n",
        "          continue\n",
        "        article_id = parts[0]\n",
        "        raw_text = parts[1]\n",
        "\n",
        "        # 全部转为小写字母\n",
        "        processed_text = raw_text.lower()\n",
        "\n",
        "        # 去除标点符号\n",
        "        processed_text = raw_text.translate(translator)\n",
        "\n",
        "        # 3. 移除所有空格（包括单词间的空格、换行符等）\n",
        "        # ' ''.join(processed_text.split())' 可以保留一个空格，\n",
        "        # 但根据要求 \"Remove all whitespace so that all words are concatenated\"，应移除所有空格\n",
        "        processed_text = processed_text.replace(' ', '')\n",
        "        processed_text = processed_text.replace('\\t', '')\n",
        "        processed_text = processed_text.replace('\\n', '')\n",
        "\n",
        "        data_list.append((article_id, processed_text))\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"文件 '{filename}' 不存在。\")\n",
        "  except Exception as e:\n",
        "    print(f\"发生错误：{e}\")\n",
        "\n",
        "  return data_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm_yCnRIaeKB"
      },
      "source": [
        "### 2.2 Part IB: Document shingles (Text)\n",
        "Write a function `shingle_document` that takes a processed article string and a parameter `k` and shards the document as follows:\n",
        "- Each substring of length $k$ in the document is hashed to a 32-bit integer (refer to the `crc32` function in Python's `binascii` library: [https://docs.python.org/3/library/binascii.html](https://docs.python.org/3/library/binascii.html)).\n",
        "- Return a list of the unique 32-bit integers obtained in the previous step (use Python sets for deduplication)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ydjZVkvNaeKC"
      },
      "outputs": [],
      "source": [
        "def shingle_document(string, k):\n",
        "  # initialize set data structure\n",
        "  # for each position in string,\n",
        "    # extract substring of length k\n",
        "    # hash substring into 32-bit integer using crc32\n",
        "    # insert into set\n",
        "  # return set\n",
        "  import binascii\n",
        "  from typing import Set, List\n",
        "\n",
        "  shingle_hashes = set()\n",
        "\n",
        "  # 遍历字符串以提取所有长度为 k 的子字符串（Shingle）\n",
        "  # 循环范围从 0 到 len(string) - k\n",
        "  for i in range(len(string) - k + 1):\n",
        "      # 1. 提取 Shingle\n",
        "      shingle = string[i:i + k]\n",
        "\n",
        "      # 2. 将 Shingle 编码为字节串 (bytes)，因为 crc32 期望字节输入\n",
        "      shingle_bytes = shingle.encode('utf-8')\n",
        "\n",
        "      # 3. 计算 CRC-32 哈希值，并将其添加到集合中进行去重\n",
        "      # crc32 返回一个非负整数\n",
        "      hash_value = binascii.crc32(shingle_bytes)\n",
        "      shingle_hashes.add(hash_value)\n",
        "\n",
        "  # 4. 返回包含唯一哈希值的列表\n",
        "  return list(shingle_hashes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60XEUFD8aeKC"
      },
      "source": [
        "### 2.3 Part IC: Jaccard Similarity (Text)\n",
        "Write a function `jaccard` that takes two sharded documents and computes their Jaccard distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LzNdIX2RaeKC"
      },
      "outputs": [],
      "source": [
        "def jaccard(a, b):\n",
        "  # compute union size\n",
        "  # compute intersection size\n",
        "  # return ratio of union and intersection\n",
        "  set_a = set(a)\n",
        "  set_b = set(b)\n",
        "\n",
        "  # 计算交集 (Intersection)\n",
        "  # set_a.intersection(set_b) 或 set_a & set_b\n",
        "  intersection_size = len(set_a.intersection(set_b))\n",
        "\n",
        "  # 计算并集 (Union)\n",
        "  # set_a.union(set_b) 或 set_a | set_b\n",
        "  union_size = len(set_a.union(set_b))\n",
        "\n",
        "  if union_size == 0:\n",
        "    # 并集为0表示意味着两个集合都为空，相似度为 1.0\n",
        "    return 1.0\n",
        "\n",
        "  jaccard_similarity = intersection_size / union_size\n",
        "\n",
        "  return intersection_size / union_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su_TGI6WaeKC"
      },
      "source": [
        "### 2.4 Part ID: Put these together (Text)\n",
        "Write a function that uses the above functions to perform the following operations:\n",
        "1. Parse a data file.\n",
        "2. Return a list of tuples, each tuple in the format `(id1, id2, s)`, where `id1` and `id2` are document ids and `s` is the computed Jaccard similarity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pairwise_jaccard_from_file(data_path, k):\n",
        "    docs = parse_data(data_path)  # [(id, processed_text)]\n",
        "    shingles_by_id = {doc_id: shingle_document(txt, k) for doc_id, txt in docs}\n",
        "\n",
        "    res = []\n",
        "    ids = [doc_id for doc_id, _ in docs]\n",
        "    for i in range(len(ids)):\n",
        "        for j in range(i + 1, len(ids)):\n",
        "            id1, id2 = ids[i], ids[j]\n",
        "            s = jaccard(shingles_by_id[id1], shingles_by_id[id2])\n",
        "            res.append((id1, id2, s))\n",
        "    return res"
      ],
      "metadata": {
        "id": "gVP9XVBi-OI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEN9_1CqaeKC"
      },
      "source": [
        "### 2.5 Part IE: Experiment 0 (Text)\n",
        "Use your function to answer the following question:\n",
        "**What is the effect of sharding length `k` on the Jaccard similarity of plagiarism instances versus instances that are not plagiarized?**\n",
        "\n",
        "Experiment requirements:\n",
        "- Make a plot with $k$ on the x-axis and average Jaccard similarity on the y-axis.\n",
        "- Plot two lines: one for plagiarism instances and one for non-plagiarism instances.\n",
        "- Use the 1000-document dataset for this experiment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, string, binascii, math, csv\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def read_pairs_with_labels(pairs_file):\n",
        "  import csv\n",
        "\n",
        "  if not os.path.exists(pairs_file):\n",
        "    raise FileNotFoundError(\"Pairs file not found: {}\".format(pairs_file))\n",
        "\n",
        "  with open(pairs_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    sample = f.read(2048)\n",
        "    f.seek(0)\n",
        "    try:\n",
        "      dialect = csv.Sniffer().sniff(sample)\n",
        "    except Exception:\n",
        "      dialect = csv.excel_tab if \"\\t\" in sample else csv.excel\n",
        "    rows = list(csv.reader(f, dialect))\n",
        "\n",
        "  if rows:\n",
        "    head = [c.lower() for c in rows[0]]\n",
        "    if len(head) > 2 and ((\"id1\" in head[0]) or (\"doc1\" in head[0])) and ((\"id2\" in head[1]) or (\"doc2\" in head[1])):\n",
        "      rows = rows[1:]\n",
        "\n",
        "  out = []\n",
        "  for r in rows:\n",
        "    # 如果r的长度小于2继续\n",
        "    if len(r) < 2:\n",
        "      continue\n",
        "    id1 = r[0].strip()\n",
        "    id2 = r[1].strip()\n",
        "    label = r[2].stiip().lower() if len(r) > 2 else \"unknown\"\n",
        "    if not id1 or not id2:\n",
        "      continue\n",
        "    if label.startwith(\"plag\"):\n",
        "      label_norm = \"plagiarized\"\n",
        "    elif label in {\"non\", \"negative\", \"not\", \"clean\", \"no\"}:\n",
        "      label_norm = \"non\"\n",
        "    else:\n",
        "      label_norm = \"non\" if label != \"unknown\" else \"unknown\"\n",
        "    out.append((id1, id2, label_norm))\n",
        "  return out\n",
        "\n",
        "\n",
        "def experiment_0_k_vs_jaccard(data_path, pairs_path, k_values, save_plot_path=\"exp0_k_vs_jaccard.png\"):\n",
        "    \"\"\"\n",
        "    仅对“标注对”计算 Jaccard 并按标签求平均，避免 O(N^2)。\n",
        "    产出折线图：横轴 k，纵轴 平均 Jaccard（抄袭 / 非抄袭 两条曲线）。\n",
        "    返回 (results_dict, figure_path)，其中 results_dict 形如：\n",
        "      {k: {'plagiarized': avg, 'non': avg}}\n",
        "    \"\"\"\n",
        "    docs = dict(parse_data(data_path))  # id -> processed text\n",
        "    pairs = _read_pairs_with_labels(pairs_path)\n",
        "\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        # 只为成对出现的文档构建 shingles（更高效）\n",
        "        needed = set()\n",
        "        for id1, id2, _ in pairs:\n",
        "            if id1 in docs and id2 in docs:\n",
        "                needed.add(id1); needed.add(id2)\n",
        "        shingles = {doc_id: shingle_document(docs[doc_id], k) for doc_id in needed}\n",
        "\n",
        "        sums, cnts = defaultdict(float), defaultdict(int)\n",
        "        for id1, id2, label in pairs:\n",
        "            if label == \"unknown\":\n",
        "                continue\n",
        "            if id1 not in shingles or id2 not in shingles:\n",
        "                continue\n",
        "            s = jaccard(shingles[id1], shingles[id2])\n",
        "            sums[label] += s\n",
        "            cnts[label] += 1\n",
        "\n",
        "        results[k] = {\n",
        "            \"plagiarized\": (sums[\"plagiarized\"] / cnts[\"plagiarized\"]) if cnts[\"plagiarized\"] else float(\"nan\"),\n",
        "            \"non\": (sums[\"non\"] / cnts[\"non\"]) if cnts[\"non\"] else float(\"nan\"),\n",
        "        }\n",
        "\n",
        "    # 画图\n",
        "    ks = sorted(results.keys())\n",
        "    y_plag = [results[k][\"plagiarized\"] for k in ks]\n",
        "    y_non  = [results[k][\"non\"] for k in ks]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(ks, y_plag, marker=\"o\", label=\"Plagiarized\")\n",
        "    plt.plot(ks, y_non,  marker=\"o\", label=\"Non-plagiarized\")\n",
        "    plt.xlabel(\"k (shingle length)\")\n",
        "    plt.ylabel(\"Average Jaccard similarity\")\n",
        "    plt.title(\"Experiment 0: Effect of k on Average Jaccard Similarity\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_plot_path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    return results, save_plot_path"
      ],
      "metadata": {
        "id": "rDRX1S02-4B5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysV5tVmNaeKC"
      },
      "source": [
        "## 3. Part II: MinHash\n",
        "**DUE**: Tuesday Sept. 18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd63zS97aeKC"
      },
      "source": [
        "### 3.1 Experiment Overview (Text)\n",
        "In this section, you will implement the MinHash algorithm and perform an experiment to see how well it estimates Jaccard similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frqIvM1WaeKC"
      },
      "source": [
        "### 3.2 Part IIA: Prepare shingles for processing (Text)\n",
        "Implement a function that takes the shingled documents and returns a list of item-document pairs sorted by items, which will be used to compute the minhash signature of each document. Note that due to the shingling logic we used above, we represent items as 32-bit integers. The function specifications are as follows:\n",
        "- Input: A list of tuples in the form `(docid, [items])`.\n",
        "- Output: A tuple containing two elements:\n",
        "  1. A list of tuples in the form `(item, docid)`, which contains one entry for each item appearing in each document.\n",
        "  2. A list of document ids found in the dataset.\n",
        "- The output list of tuples should be sorted by item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-BTPRywTaeKD"
      },
      "outputs": [],
      "source": [
        "def invert_shingles(shingled_documents):\n",
        "  # initialize list for tuples\n",
        "  # initialize list for document ids\n",
        "  # for each document in input\n",
        "    # append document id to list\n",
        "\n",
        "    # for each item in document\n",
        "      # append (item, docid) tuple\n",
        "\n",
        "  # sort tuple list\n",
        "  # return sorted tuple list, and document list\n",
        "  inv = []\n",
        "  docids = []\n",
        "  seen_docs = set()\n",
        "\n",
        "  for docid, items in shingled_documents:\n",
        "    if docids not in seen_docs:\n",
        "      docids.append(docid)\n",
        "      seen_docs.add(docid)\n",
        "  # 每个 item 在该 doc 出现一条记录（若 items 里有重复，去重以免虚高）\n",
        "    for it in set(items):\n",
        "      inv.append((it, docid))\n",
        "  inv.sort(k=lambda x: (x[0], x[1]))\n",
        "  return inv, docids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7JdYmhMaeKD"
      },
      "source": [
        "### 3.3 Part IIB: Generate hash functions (Text)\n",
        "Use the `make_random_hash_fn` function below to implement the `make_hashes` function. Given the input `num_hashes`, this function will return a list of hash functions that mimic the random permutation approach used in Minhash calculation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_random_hash_fn(a, b, p):\n",
        "    def h(x):\n",
        "        return (a * x + b) % p\n",
        "    return h\n",
        "\n",
        "\n",
        "def make_hashes(num_hashes, seed=42, prime=4294967311):\n",
        "    rng = random.Random(seed)\n",
        "    hashes = []\n",
        "    for _ in range(num_hashes):\n",
        "        a = rng.randrange(1, prime - 1)\n",
        "        b = rng.randrange(0, prime - 1)\n",
        "        hashes.append(make_random_hash_fn(a, b, prime))\n",
        "    return hashes"
      ],
      "metadata": {
        "id": "lC8oPa0Px9pm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrfcpK0caeKD"
      },
      "source": [
        "### 3.4 Part IIC: Construct the Minhash Signature Matrix (Text)\n",
        "Implement a function that builds the Minhash signature matrix. You can use the following code as a starting point, which refers to the functions you implemented above and outlines the construction algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-Xw1ak3AaeKD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def make_minhash_signature(shingled_data, num_hashes, seed=42):\n",
        "  inv_index, docids = invert_shingles(shingled_data)\n",
        "  num_docs = len(docids)\n",
        "\n",
        "  # initialize the signature matrix with infinity in every entry\n",
        "  sigmatrix = np.full([num_hashes, num_docs], np.inf)\n",
        "\n",
        "  col_index = {d: j for j, d in enumerate(docids)}\n",
        "\n",
        "  # generate hash functions\n",
        "  hash_funcs = make_hashes(num_hashes)\n",
        "\n",
        "  # iterate over each non-zero entry of the characteristic matrix\n",
        "  for row, docid in inv_index:\n",
        "    # update signature matrix if needed\n",
        "    # THIS IS WHAT YOU NEED TO IMPLEMENT\n",
        "    j = col_index[docid]\n",
        "    for r, h in enumerate(hash_funcs):\n",
        "      hv = h(item)\n",
        "      if hv < sigmatrix[r, j]:\n",
        "        sigmatrix[r, j] = hv\n",
        "  sigmatrix = sigmatrix.astype(np.uint64)\n",
        "  return sigmatrix, docids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMy6ThBBaeKD"
      },
      "source": [
        "### 3.5 Part IID: MinHash similarity estimate (Text)\n",
        "Write a function that computes the similarity of two documents using the minhash matrix computed above. The function specifications are as follows:\n",
        "- Input:\n",
        "  1. `id1, id2`: Document ids.\n",
        "  2. `minhash_sigmat`: Minhash signature matrix.\n",
        "  3. `docids`: List of document ids, used to index the columns of the minhash signature matrix.\n",
        "- Output: Jaccard similarity estimated using minhash.\n",
        "\n",
        "Hint: Refer to the example code for comparing numpy arrays below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LZ9OI0cBaeKD"
      },
      "outputs": [],
      "source": [
        "def minhash_similarity(id1, id2, minhash_sigmat, docids):\n",
        "  # get column of the similarity matrix for the two documents\n",
        "  # calculate the fraction of rows where two columns match\n",
        "  # return this fraction as the minhash similarity estimate\n",
        "  col_index = {d: j for j, d in enumerate(docids)}\n",
        "  if id1 not in col_index or id2 not in col_index:\n",
        "      raise KeyError(\"Document id not found in docids.\")\n",
        "  j1 = col_index[id1]\n",
        "  j2 = col_index[id2]\n",
        "  col1 = minhash_sigmat[:, j1]\n",
        "  col2 = minhash_sigmat[:, j2]\n",
        "  matches = np.sum(col1 == col2)\n",
        "  return float(matches) / float(minhash_sigmat.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-sKHffSaeKD"
      },
      "source": [
        "### 3.6 Part IIE: Put these together (Text)\n",
        "Write a function that takes shingled documents and computes the Minhash estimated similarities between each pair of documents. This will be similar to your function for Part ID."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def minhash_all_pairs(shingled_data, num_hashes, seed=42):\n",
        "    \"\"\"\n",
        "    return [(id1, id2, est), ...]\n",
        "    \"\"\"\n",
        "    sig, docids = make_minhash_signature(shingled_data, num_hashes, seed=seed)\n",
        "    res = []\n",
        "    n = len(docids)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            id1, id2 = docids[i], docids[j]\n",
        "            est = minhash_similarity(id1, id2, sig, docids)\n",
        "            res.append((id1, id2, est))\n",
        "    return res, sig, docids"
      ],
      "metadata": {
        "id": "wJE2l6YvJaW8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBVFWkcRaeKD"
      },
      "source": [
        "### 3.7 Part IIF: Experiment 1 (Text)\n",
        "Use your function to carry out the following experiment:\n",
        "**What is the effect of the number of hash functions used to compute the Minhash signature on the accuracy of the Minhash estimate of Jaccard similarity?**\n",
        "\n",
        "Experiment requirement: Carry out this experiment on the 1000-document dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 3.7 IIF：实验 1 —— 哈希个数对估计精度的影响 ----------\n",
        "def experiment_1_minhash_accuracy(\n",
        "    shingled_data,\n",
        "    true_jaccard_lookup=None,\n",
        "    num_hashes_list=(10, 20, 40, 80, 120, 160, 200),\n",
        "    max_pairs=None,\n",
        "    seed=42,\n",
        "    save_plot_path=\"exp1_numhash_vs_error.png\"\n",
        "):\n",
        "    \"\"\"\n",
        "    输入:\n",
        "      shingled_data       : [(docid, [items]), ...]（items 建议去重）\n",
        "      true_jaccard_lookup : 可选，dict[(id1,id2)] = 真实 Jaccard（若为空则现场计算）\n",
        "      num_hashes_list     : 需要测试的哈希函数个数列表\n",
        "      max_pairs           : 可选，若文档很多，可随机抽样若干对计算，控制时间\n",
        "      seed                : 随机种子（用于抽样与哈希）\n",
        "      save_plot_path      : 图片保存路径\n",
        "\n",
        "    输出:\n",
        "      results: {H: {\"mae\": 平均绝对误差, \"rmse\": 均方根误差, \"corr\": 相关系数}}，并画图（H vs MAE）\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "    # 文档 id 列表与所有对\n",
        "    docids = [docid for docid, _ in shingled_data]\n",
        "    pairs = []\n",
        "    for i in range(len(docids)):\n",
        "        for j in range(i + 1, len(docids)):\n",
        "            pairs.append((docids[i], docids[j]))\n",
        "\n",
        "    # 抽样\n",
        "    if max_pairs is not None and max_pairs < len(pairs):\n",
        "        pairs = rng.sample(pairs, max_pairs)\n",
        "\n",
        "    # 若未提供真实 Jaccard，则现场计算一次\n",
        "    if true_jaccard_lookup is None:\n",
        "        id2items = {docid: set(items) for docid, items in shingled_data}\n",
        "        true_jaccard_lookup = {}\n",
        "        for id1, id2 in pairs:\n",
        "            s = jaccard(id2items[id1], id2items[id2])\n",
        "            true_jaccard_lookup[(id1, id2)] = s\n",
        "\n",
        "    results = {}\n",
        "    # 逐个 H 做 MinHash，并评估误差\n",
        "    for H in num_hashes_list:\n",
        "        sig, dlist = make_minhash_signature(shingled_data, H, seed=seed)\n",
        "        col_index = {d: j for j, d in enumerate(dlist)}\n",
        "\n",
        "        est_vals = []\n",
        "        true_vals = []\n",
        "\n",
        "        for id1, id2 in pairs:\n",
        "            if id1 not in col_index or id2 not in col_index:\n",
        "                continue\n",
        "            est = minhash_similarity(id1, id2, sig, dlist)\n",
        "            true = true_jaccard_lookup[(id1, id2)]\n",
        "            est_vals.append(est)\n",
        "            true_vals.append(true)\n",
        "\n",
        "        est_arr = np.array(est_vals, dtype=float)\n",
        "        true_arr = np.array(true_vals, dtype=float)\n",
        "        if len(est_arr) == 0:\n",
        "            mae = np.nan\n",
        "            rmse = np.nan\n",
        "            corr = np.nan\n",
        "        else:\n",
        "            mae = np.mean(np.abs(est_arr - true_arr))\n",
        "            rmse = np.sqrt(np.mean((est_arr - true_arr) ** 2))\n",
        "            # 皮尔逊相关\n",
        "            if np.std(est_arr) == 0 or np.std(true_arr) == 0:\n",
        "                corr = np.nan\n",
        "            else:\n",
        "                corr = np.corrcoef(est_arr, true_arr)[0, 1]\n",
        "\n",
        "        results[H] = {\"mae\": mae, \"rmse\": rmse, \"corr\": corr}\n",
        "\n",
        "    # 画图：H vs MAE\n",
        "    Hs = sorted(results.keys())\n",
        "    maes = [results[h][\"mae\"] for h in Hs]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(Hs, maes, marker=\"o\")\n",
        "    plt.xlabel(\"Number of hash functions (H)\")\n",
        "    plt.ylabel(\"MAE vs true Jaccard\")\n",
        "    plt.title(\"Experiment 1: Effect of H on MinHash Estimation Error\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_plot_path, dpi=200)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    return results, save_plot_path\n",
        "\n",
        "data_path = \"/content/sample_data/MinHash/data/articles_10000.train\"\n",
        "k = 5  # 选一个合适的 k\n",
        "docs = parse_data(data_path)  # [(id, processed_text)]\n",
        "shingled_data = [(docid, list(set(shingle_document(text, k)))) for docid, text in docs]\n",
        "\n",
        "# 跑实验：测试多种哈希个数\n",
        "num_hashes_list = [10, 20, 40, 80, 120, 160, 200]\n",
        "results, fig_path = experiment_1_minhash_accuracy(\n",
        "    shingled_data,\n",
        "    num_hashes_list=num_hashes_list,\n",
        "    max_pairs=50000  # 可选：限制比较的文档对数量，加快速度\n",
        ")\n",
        "print(results)\n",
        "print(\"Saved figure to:\", fig_path)"
      ],
      "metadata": {
        "id": "U_gIYCkNKEiU",
        "outputId": "b87835e3-164e-46fc-9de6-587d8fc13e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-665546268.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# 跑实验：测试多种哈希个数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mnum_hashes_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m results, fig_path = experiment_1_minhash_accuracy(\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mshingled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mnum_hashes_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_hashes_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-665546268.py\u001b[0m in \u001b[0;36mexperiment_1_minhash_accuracy\u001b[0;34m(shingled_data, true_jaccard_lookup, num_hashes_list, max_pairs, seed, save_plot_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# 逐个 H 做 MinHash，并评估误差\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mH\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_hashes_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_minhash_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshingled_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mcol_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4049911242.py\u001b[0m in \u001b[0;36mmake_minhash_signature\u001b[0;34m(shingled_data, num_hashes, seed)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_minhash_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshingled_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hashes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0minv_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvert_shingles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshingled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mnum_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-350157466.py\u001b[0m in \u001b[0;36minvert_shingles\u001b[0;34m(shingled_documents)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdocid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshingled_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdocids\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen_docs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mdocids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mseen_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5V2INA2aeKD"
      },
      "source": [
        "## 4. Part III: Locality-Sensitive Hashing\n",
        "**DUE**: Tuesday Sept. 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zbeX6WlaeKD"
      },
      "source": [
        "### 4.1 Implement LSH (Text)\n",
        "Write a function that implements locality sensitive hashing. Function specifications:\n",
        "- Input:\n",
        "  1. `minhash_sigmatrix`: A minhash signature matrix.\n",
        "  2. `numhashes`: Number of hash functions used to construct the minhash signature matrix.\n",
        "  3. `docids`: List of document ids.\n",
        "  4. `threshold`: A minimum Jaccard similarity threshold.\n",
        "- Output: A list of hash tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPVnm64_aeKD"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def do_lsh(minhash_sigmatrix, numhashes, docids, threshold):\n",
        "  # choose the number of bands, and rows per band to use in LSH\n",
        "  b, _ = choose_nbands(threshold, numhashes)\n",
        "  r = int(numhashes / b)\n",
        "\n",
        "  narticles = len(docids)\n",
        "\n",
        "  # generate a random hash function that takes vectors of length r as input\n",
        "  hash_func = _make_vector_hash(r)\n",
        "\n",
        "  # setup the list of hashtables, will be populated with one hashtable per band\n",
        "  buckets = []\n",
        "\n",
        "  # fill hash tables for each band\n",
        "  for band in range(b):\n",
        "    # figure out which rows of minhash signature matrix to hash for this band\n",
        "    start_index = int(band * r)\n",
        "    end_index = min(start_index + r, numhashes)\n",
        "\n",
        "    # initialize hashtable for this band\n",
        "    cur_buckets = defaultdict(list)\n",
        "\n",
        "    for j in range(narticles):\n",
        "      # THIS IS WHAT YOU NEED TO IMPLEMENT\n",
        "\n",
        "    # add this hashtable to the list of hashtables\n",
        "    buckets.append(cur_buckets)\n",
        "\n",
        "  return buckets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLn3tdPvaeKE"
      },
      "source": [
        "### 4.2 Find candidate similar article pairs (Text)\n",
        "Write a function that uses the result of your LSH function and returns a list of candidate article pairs. Specifications:\n",
        "- Input: The result of `do_lsh`.\n",
        "- Output: A list of tuples `(docid1, docid2)`, each being a candidate similar article pair according to LSH."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH-VOT9baeKE"
      },
      "source": [
        "### 4.3 Experiment 2: LSH sensitivity (Text)\n",
        "Use these functions to compute the sensitivity and specificity of LSH as a function of the threshold. Use the 10,000-document dataset to perform this experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZbLteWHaeKE"
      },
      "source": [
        "## 5. Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prUQhbITaeKE"
      },
      "source": [
        "### 5.1 Obtaining data (Text)\n",
        "You can use the following Python code to download data for the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MFf4MK6aeKE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from six.moves import urllib\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/chrisjmccormick/MinHash/master/data\"\n",
        "PLAGIARISM_PATH = \"datasets/plagiarism\"\n",
        "DATA_SIZES = [100,1000,2500,10000]\n",
        "\n",
        "def fetch_data(download_root=DOWNLOAD_ROOT,\n",
        "               plagiarism_path=PLAGIARISM_PATH,\n",
        "               data_sizes=DATA_SIZES,\n",
        "               maxsize=1000):\n",
        "  if not os.path.isdir(plagiarism_path):\n",
        "      os.makedirs(plagiarism_path)\n",
        "  for size in data_sizes:\n",
        "      if size <= maxsize:\n",
        "          train_file = \"articles_\" + str(size) + \".train\"\n",
        "          train_path = plagiarism_path + '/' + train_file\n",
        "          if not os.path.exists(train_path):\n",
        "              train_url = download_root + '/' + train_file\n",
        "              urllib.request.urlretrieve(train_url, train_path)\n",
        "\n",
        "          truth_file = \"articles_\" + str(size) + \".truth\"\n",
        "          truth_path = plagiarism_path + '/' + truth_file\n",
        "          if not os.path.exists(truth_path):\n",
        "              truth_url = download_root + \"/\" + truth_file\n",
        "              urllib.request.urlretrieve(truth_url, truth_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdEXTzL8aeKE"
      },
      "source": [
        "**Usage Instructions** (Text):\n",
        "The `fetch_data` function will download data to the subdirectory pointed to by `PLAGIARISM_PATH`. You can assign the path where you want to store your data to this variable.\n",
        "The `maxsize` parameter is used to limit the size of the downloaded data. For testing and development, it is recommended to use the 1000-document dataset, which can be obtained by calling `fetch_data(maxsize=1000)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPRn1h2PaeKE"
      },
      "source": [
        "### 5.2 Generating random hash functions (Text)\n",
        "This function generates a random hash function suitable for mimicking permutations over 32-bit integers. Recall that since we are using `crc32` to represent items, we need random hash functions that generate 32-bit numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5VS8336aeKE"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def make_random_hash_fn(p=2**33-355, m=4294967295):\n",
        "    a = random.randint(1,p-1)\n",
        "    b = random.randint(0, p-1)\n",
        "    return lambda x: ((a * x + b) % p) % m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjJBRBeVaeKE"
      },
      "source": [
        "**Usage Example** (Text):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSqpXmI0aeKE"
      },
      "outputs": [],
      "source": [
        "hash_fn = make_random_hash_fn()\n",
        "hash_fn(12345)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1RBEnYUaeKE"
      },
      "source": [
        "**Explanation** (Text):\n",
        "This implements a universal hash function for 32-bit integers, which ensures that the result corresponds to the permutation of rows of the characteristic matrix required by Minhash (Refer to: [https://en.wikipedia.org/wiki/Universal_hashing](https://en.wikipedia.org/wiki/Universal_hashing)). Here, `m` is the largest number returned by `crc32`, and `p` is a prime number larger than `m`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb1FpXKuaeKE"
      },
      "source": [
        "### 5.3 Comparing numpy vectors (Text)\n",
        "The following code snippet shows how to use numpy to compare two integer vectors, which is required to compute the minhash similarity estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMBgeb8FaeKE"
      },
      "outputs": [],
      "source": [
        "# generate two vectors of length 50 with random integers from 0 to 100\n",
        "a = np.random.randint(100, size=50)\n",
        "b = np.random.randint(100, size=50)\n",
        "\n",
        "# compute the fraction of entries in which two vectors are equal\n",
        "np.mean(a == b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq6NubEjaeKK"
      },
      "source": [
        "### 5.4 Choosing the number of bands for LSH (Text)\n",
        "Given a similarity threshold, we need to choose the number of bands to use in LSH. You can use the following function to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_OQA2KNaeKK"
      },
      "outputs": [],
      "source": [
        "import scipy.optimize as opt\n",
        "import math\n",
        "\n",
        "def _choose_nbands(t, n):\n",
        "    def _error_fun(x):\n",
        "        cur_t = (1/x[0])**(x[0]/n)\n",
        "        return (t-cur_t)**2\n",
        "\n",
        "    opt_res = opt.minimize(error_fun, x0=(10), method='Nelder-Mead')\n",
        "    b = int(math.ceil(res['x'][0]))\n",
        "    r = int(n / b)\n",
        "    final_t = (1/b)**(1/r)\n",
        "    return b, final_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOp5AkQ0aeKK"
      },
      "source": [
        "### 5.5 Hashing a vector (Text)\n",
        "In LSH, we need to hash the `r` hash values of each document for each band. You can use the following function to generate a hash function for vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBlVEfW5aeKK"
      },
      "outputs": [],
      "source": [
        "def _make_vector_hash(num_hashes, m=4294967295):\n",
        "    hash_fns = make_hashes(num_hashes)\n",
        "    def _f(vec):\n",
        "      acc = 0\n",
        "      for i in range(len(vec)):\n",
        "        h = hash_fns[i]\n",
        "        acc += h(vec[i])\n",
        "      return acc % m\n",
        "    return _f"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}